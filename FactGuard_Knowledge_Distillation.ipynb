{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **NOTE**:\n",
        "\n",
        "1. This notebook submission does not include outputs: the distillation process can take > 8 hours to complete a single dataset (we used two - squad and fever). This notebook was revised after the distillation dataset creation to provide better structure to the reader.\n",
        "\n",
        "2. The datasets are available on huggingface. Please be advised that running this will require a gemini account and will accure monetary cost per request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydi4XEjI4qxn"
      },
      "source": [
        "## Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpYrLiYd2gX1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABDNnaMpwnhu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, userdata\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.genai.errors import APIError\n",
        "from typing import List, Dict, Any, Optional, Set\n",
        "import time\n",
        "from random import choice\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "from pydantic import BaseModel, Field, constr\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlm-njQwb0K5",
        "outputId": "a805f489-d869-4935-ab48-128bc2bfc9ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfASDl9Qw5sN"
      },
      "outputs": [],
      "source": [
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['GEMINI_API_KEY'] = userdata.get('GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjr92y24vOn"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkjssH8B9Z6W"
      },
      "outputs": [],
      "source": [
        "def checkpoint(dataset: pd.DataFrame, path: str, filename: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    dataset.to_csv(\n",
        "        f\"{path}/{filename}\",\n",
        "        mode='a',\n",
        "        header=not os.path.exists(f\"{path}/{filename}\") or os.path.getsize(f\"{path}/{filename}\") == 0,\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "def load_checkpoint(path: str, filename: str) -> Optional[pd.DataFrame]:\n",
        "    filepath = f\"{path}/{filename}\"\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"Checkpoint found! Loading data from {filepath}\")\n",
        "        return pd.read_csv(filepath)\n",
        "    print(f\"No checkpoint found at {filepath}. Starting fresh.\")\n",
        "    return None\n",
        "\n",
        "def initialize_checkpoint_df(\n",
        "    checkpoint_df: Optional[pd.DataFrame],\n",
        "    expected_columns: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    if checkpoint_df is None or checkpoint_df.empty:\n",
        "        return pd.DataFrame(columns=expected_columns)\n",
        "    return checkpoint_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJZJOK555zKJ"
      },
      "source": [
        "## [FEVER] Knowledge Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnVnMRr27xfX"
      },
      "outputs": [],
      "source": [
        "## Used to keep training and test sets consistent\n",
        "def prepare_fever():\n",
        "  fever_datasets = load_dataset(\"fever\", \"v1.0\", trust_remote_code=True)\n",
        "  train_dataset = fever_datasets['train']\n",
        "  target_labels = [\"SUPPORTS\", \"REFUTES\"]\n",
        "  filtered_train_dataset = train_dataset.filter(\n",
        "      lambda example: example['label'] in target_labels\n",
        "  )\n",
        "  split_datasets = filtered_train_dataset.train_test_split(\n",
        "    test_size=0.2,\n",
        "    seed=42\n",
        "    )\n",
        "  final_dataset_dict = DatasetDict({\n",
        "      'train': split_datasets['train'],\n",
        "      'test': split_datasets['test']\n",
        "  })\n",
        "  final_dataset_dict.push_to_hub(\"FEVER\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37_Wr-Rq6aDg"
      },
      "source": [
        "### Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLgmZKHQ6Z5H"
      },
      "outputs": [],
      "source": [
        "class FEVERDistillationPoint(BaseModel):\n",
        "    claim: constr(min_length=1) = Field(description=\"The original FEVER claim statement.\")\n",
        "    label: constr(min_length=1) = Field(description=\"The original FEVER label: 'SUPPORTS' or 'REFUTES'.\")\n",
        "    context: constr(min_length=1) = Field(description=\"The comprehensive evidence/document that fully and explicitly verifies or contradicts the claim.\")\n",
        "    rationale: constr(min_length=1) = Field(description=\"A step-by-step reasoning/rationale generated by the teacher model explaining the verdict based on the context.\")\n",
        "    verdict: bool = Field(description=\"True if the claim is SUPPORTS, False if REFUTES.\")\n",
        "\n",
        "class FEVERDistilledBatch(BaseModel):\n",
        "    results: List[FEVERDistillationPoint] = Field(description=\"A list containing the structured fact-checking analysis for every claim in the input.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_wZgiRn7f_3"
      },
      "outputs": [],
      "source": [
        "FEVER_OUTPUT_COLUMNS = ['original_fever_id', 'claim', 'label', 'context', 'rationale', 'verdict']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upVL8_IG6Qf8"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE3sFh2t6iPL"
      },
      "outputs": [],
      "source": [
        "def _construct_fever_prompt_and_config(\n",
        "    fever_batch_data: List[Dict[str, str]],\n",
        "    teacher_model_name: str\n",
        ") -> tuple[str, str, types.GenerateContentConfig]:\n",
        "\n",
        "    system_instruction = textwrap.dedent(\"\"\"\n",
        "        You are an expert fact-checking training data synthesizer. Your task is to generate the appropriate 'context' (evidence) and a 'rationale' for each provided FEVER claim, focusing only on 'SUPPORTS' and 'REFUTES' labels.\n",
        "\n",
        "        **FOR 'SUPPORTS' CLAIMS:** Generate a complete paragraph for the 'context' that **fully and explicitly verifies** the claim. The 'rationale' must clearly state how the context proves the claim. Set 'verdict' to true.\n",
        "        **FOR 'REFUTES' CLAIMS:** Generate a complete paragraph for the 'context' that **fully and explicitly contradicts** the claim. The 'rationale' must clearly state which specific fact in the context refutes the claim. Set 'verdict' to false.\n",
        "\n",
        "        All generated contexts must be coherent and informative, and all claims, labels, and rationales must be non-empty strings.\n",
        "    \"\"\").strip()\n",
        "\n",
        "    input_entries = []\n",
        "    for i, entry in enumerate(fever_batch_data):\n",
        "        entry_text = textwrap.dedent(f\"\"\"\n",
        "            --- FEVER Entry {i+1} ---\n",
        "            CLAIM: {entry.get('claim', 'N/A')}\n",
        "            ORIGINAL LABEL: {entry.get('label', 'N/A')}\n",
        "        \"\"\").strip()\n",
        "        input_entries.append(entry_text)\n",
        "\n",
        "    output_schema_dict = FEVERDistilledBatch.model_json_schema()\n",
        "\n",
        "    prompt = (\n",
        "        f\"Process the following {len(fever_batch_data)} binary FEVER entries. Your final output MUST be a single JSON object that strictly conforms to the FEVERDistilledBatch schema. \"\n",
        "        \"The 'results' list must contain one fully generated entry for each input claim. Do not include any text outside the JSON object.\\n\"\n",
        "        + \"\\n\\n\".join(input_entries)\n",
        "    )\n",
        "\n",
        "    config = types.GenerateContentConfig(\n",
        "            system_instruction=system_instruction,\n",
        "            response_mime_type=\"application/json\",\n",
        "            response_schema=output_schema_dict,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "    return system_instruction, prompt, config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcvY-Hl06rnB"
      },
      "outputs": [],
      "source": [
        "def generate_fever_distillation_data(\n",
        "    fever_batch_data: List[Dict[str, str]],\n",
        "    teacher_model_name: str = 'gemini-2.5-pro'\n",
        ") -> List[Dict[str, Any]]:\n",
        "\n",
        "    if not fever_batch_data:\n",
        "        return []\n",
        "\n",
        "    system_instruction, prompt, config = _construct_fever_prompt_and_config(\n",
        "        fever_batch_data, teacher_model_name\n",
        "    )\n",
        "\n",
        "    print(f\"-> Sending request for {len(fever_batch_data)} entries to teacher model ({teacher_model_name})...\")\n",
        "\n",
        "    client = genai.Client()\n",
        "\n",
        "    try:\n",
        "      response = client.models.generate_content(\n",
        "          model=teacher_model_name,\n",
        "          contents=[system_instruction, prompt],\n",
        "          config=config\n",
        "      )\n",
        "\n",
        "      return json.loads(response.text)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"API Call Validation Exception: {e}. Returning empty results for this batch.\")\n",
        "      return {'results': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-zHVcGv65Ll"
      },
      "outputs": [],
      "source": [
        "def _process_and_flatten_fever_output(\n",
        "    distilled_batch_output: Dict[str, List[Dict[str, Any]]], # Dictionary with 'results' list\n",
        "    batch_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    if 'results' not in distilled_batch_output or not distilled_batch_output['results']:\n",
        "        print(\"API response missing 'results' list or list is empty.\")\n",
        "        return pd.DataFrame(columns=FEVER_OUTPUT_COLUMNS)\n",
        "\n",
        "    claims_list = distilled_batch_output['results']\n",
        "    batch_results = []\n",
        "    for i, distilled_point in enumerate(claims_list):\n",
        "\n",
        "        if i >= len(batch_df):\n",
        "            print(\"Error: Too many claims returned by API. Stopping processing.\")\n",
        "            break\n",
        "\n",
        "        original_fever_id = batch_df.iloc[i]['id']\n",
        "\n",
        "        try:\n",
        "            batch_results.append({\n",
        "                'original_fever_id': original_fever_id,\n",
        "                'claim': distilled_point['claim'],\n",
        "                'label': distilled_point['label'],\n",
        "                'context': distilled_point['context'],\n",
        "                'rationale': distilled_point['rationale'],\n",
        "                'verdict': distilled_point['verdict']\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Data Structure Error for FEVER ID {original_fever_id}. Skipping claim. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(batch_results, columns=FEVER_OUTPUT_COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTuV9qKd7BPA"
      },
      "outputs": [],
      "source": [
        "def distill_fever_to_dataset(\n",
        "    input_dataset: Dataset,\n",
        "    batch_size: int = 50,\n",
        "    teacher_model_name: str = 'gemini-2.5-flash',\n",
        "    checkpoint_dir: str = 'distillation_checkpoints',\n",
        "    checkpoint_filename: str = 'fever_distilled_data.csv'\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    if not input_dataset:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_full = input_dataset.to_pandas()\n",
        "\n",
        "    df_binary = df_full[df_full['label'].isin(['SUPPORTS', 'REFUTES'])].copy()\n",
        "    print(f\"Filtered out 'NOT ENOUGH INFO'. {len(df_binary)} claims remaining for distillation.\")\n",
        "\n",
        "    current_checkpoint_df = initialize_checkpoint_df(\n",
        "        load_checkpoint(checkpoint_dir, checkpoint_filename), FEVER_OUTPUT_COLUMNS\n",
        "    )\n",
        "\n",
        "    processed_fever_ids: Set[int] = set(current_checkpoint_df['original_fever_id'].astype(int).tolist())\n",
        "    print(f\"Current checkpoint size (claims): {len(current_checkpoint_df)}\")\n",
        "\n",
        "    df_unprocessed = df_binary[~df_binary['id'].isin(processed_fever_ids)].copy()\n",
        "    print(f\"Removed {len(df_binary) - len(df_unprocessed)} already processed FEVER IDs.\")\n",
        "\n",
        "    if df_unprocessed.empty:\n",
        "        print(\"All FEVER samples already processed. Returning final dataset.\")\n",
        "        return current_checkpoint_df\n",
        "\n",
        "    df_unprocessed = df_unprocessed.sample(frac=1).reset_index(drop=True)\n",
        "    total_samples = len(df_unprocessed)\n",
        "    batch_indices = np.array_split(df_unprocessed.index, np.ceil(total_samples / batch_size))\n",
        "    num_batches = len(batch_indices)\n",
        "    print(f\"FEVER IDs remaining: {total_samples} in {num_batches} batches.\")\n",
        "\n",
        "    for i, indices in tqdm(enumerate(batch_indices), total=num_batches, desc=\"Distilling FEVER Batches\"):\n",
        "        batch_df = df_unprocessed.loc[indices]\n",
        "        fever_batch_input = batch_df[['claim', 'label']].to_dict('records')\n",
        "\n",
        "        try:\n",
        "            distilled_batch_output = generate_fever_distillation_data(\n",
        "                fever_batch_data=fever_batch_input,\n",
        "                teacher_model_name=teacher_model_name\n",
        "            )\n",
        "            batch_results_df = _process_and_flatten_fever_output(\n",
        "                distilled_batch_output, batch_df\n",
        "            )\n",
        "            checkpoint(batch_results_df, checkpoint_dir, checkpoint_filename)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Critical Error in FEVER Batch {i+1}. Halting distillation. Error: {e}\")\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        final_df = pd.read_csv(f\"{checkpoint_dir}/{checkpoint_filename}\")\n",
        "        final_df = final_df.drop_duplicates(subset=['claim']).reset_index(drop=True)\n",
        "        print(f\"FEVER Distillation finished. Total final processed samples: {len(final_df)}\")\n",
        "        return final_df\n",
        "    except FileNotFoundError:\n",
        "        print(\"Final checkpoint file not found. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA2TPtpc6UgQ"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFzLG62P6WyX"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_PATH_FULL = '/content/drive/MyDrive/checkpoints/FactGuard/fever_distilled_data.csv'\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/checkpoints/FactGuard/'\n",
        "CHECKPOINT_FILENAME = 'fever_distilled_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rxLAegS6MU1"
      },
      "outputs": [],
      "source": [
        "# Use this to keep consistency across TRAIN / TEST splits.\n",
        "fever_datasets = load_dataset(\"rickpereira/FEVER\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVb4mxcO6Vyh"
      },
      "outputs": [],
      "source": [
        "distilled_dataset = distill_fever_to_dataset(\n",
        "    fever_datasets['train'],\n",
        "    checkpoint_dir = CHECKPOINT_DIR,\n",
        "    checkpoint_filename = CHECKPOINT_FILENAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y7qiTPI77Xo"
      },
      "source": [
        "### Push to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EczwRhFk77OY"
      },
      "outputs": [],
      "source": [
        "fever_dataset = load_checkpoint(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n",
        "fever_datasets = Dataset.from_pandas(fever_dataset)\n",
        "fever_dataset_dict = DatasetDict({\n",
        "    \"train\": fever_datasets\n",
        "})\n",
        "fever_dataset_dict.push_to_hub(\"factguard_fever_distilled_datasets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgKEyKMd56hP"
      },
      "source": [
        "## [SQuAD] Knowledge Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMJLqMEz8B9M"
      },
      "source": [
        "### Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuEadkGZ8Bu0"
      },
      "outputs": [],
      "source": [
        "FINAL_OUTPUT_COLUMNS = ['original_squad_id', 'claim', 'label', 'rationale', 'context']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzuvI7Jy56M5"
      },
      "outputs": [],
      "source": [
        "class FactCheckDataPoint(BaseModel):\n",
        "    claim: constr(min_length=1) = Field(description=\"The generated statement provided for fact-checking.\")\n",
        "    label: constr(min_length=1) = Field(description=\"The final classification of the claim's accuracy: 'TRUE' or 'FALSE'.\")\n",
        "    rationale: constr(min_length=1) = Field(description=\"A concise, direct, and single-sentence explanation of why the claim is true or false, citing the relevant part of the CONTEXT.\")\n",
        "    verdict: bool = Field(description=\"True if the claim is factually correct, False otherwise.\")\n",
        "\n",
        "class BatchFactCheckResult(BaseModel):\n",
        "    results: List[FactCheckDataPoint] = Field(description=\"A list containing the fact-checking entry.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcf4LdB-8LPs"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkYiePKQ8TUB"
      },
      "outputs": [],
      "source": [
        "def _get_unprocessed_squad_data(\n",
        "    input_dataset: Dataset,\n",
        "    checkpoint_dir: str,\n",
        "    checkpoint_filename: str\n",
        ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "\n",
        "    df_full = input_dataset.to_pandas()\n",
        "    df_full['squad_answer'] = df_full['answers'].apply(lambda x: x.get('text', [''])[0])\n",
        "\n",
        "    current_checkpoint_df = initialize_checkpoint_df(\n",
        "        load_checkpoint(checkpoint_dir, checkpoint_filename), FINAL_OUTPUT_COLUMNS\n",
        "    )\n",
        "\n",
        "    processed_squad_ids: Set[str] = set(current_checkpoint_df['original_squad_id'].astype(str).tolist())\n",
        "    print(f\"Current checkpoint size (claims): {len(current_checkpoint_df)}\")\n",
        "    df_unprocessed = df_full[~df_full['id'].isin(processed_squad_ids)].copy()\n",
        "    print(f\"Removed {len(df_full[df_full['id'].isin(processed_squad_ids)])} already processed SQuAD IDs.\")\n",
        "\n",
        "    return df_unprocessed, current_checkpoint_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx5v886z8WFb"
      },
      "outputs": [],
      "source": [
        "def _process_and_flatten_batch_output(\n",
        "    distilled_batch_output: List[Dict[str, Any]],\n",
        "    batch_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    batch_results = []\n",
        "    input_indices = batch_df.index.tolist()\n",
        "\n",
        "    for j, flat_claim in enumerate(distilled_batch_output):\n",
        "        original_squad_index_in_batch = j // 2\n",
        "\n",
        "        if original_squad_index_in_batch >= len(input_indices):\n",
        "            print(f\"Error: Claim index {j} outside bounds of input batch. Stopping.\")\n",
        "            break\n",
        "\n",
        "        original_row_index = input_indices[original_squad_index_in_batch]\n",
        "\n",
        "        original_id = batch_df.loc[original_row_index, 'id']\n",
        "        original_context = batch_df.loc[original_row_index, 'context']\n",
        "\n",
        "        try:\n",
        "            batch_results.append({\n",
        "                'original_squad_id': original_id,\n",
        "                'claim': flat_claim['claim'],\n",
        "                'label': flat_claim['label'],\n",
        "                'rationale': flat_claim['rationale'],\n",
        "                'context': original_context,\n",
        "                'verdict': flat_claim['verdict']\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Validation Error for SQuAD ID {original_id}. Skipping both claims. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(batch_results, columns=FINAL_OUTPUT_COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaEjgQL8AiBc"
      },
      "outputs": [],
      "source": [
        "def _construct_distillation_prompt_and_config(\n",
        "    squad_batch_data: List[Dict[str, str]],\n",
        "    teacher_model_name: str\n",
        ") -> tuple[str, str, types.GenerateContentConfig]:\n",
        "\n",
        "    system_instruction = textwrap.dedent(\"\"\"\n",
        "        You are an expert fact-checking data generator. For each provided record (Context, Question, Answer), you must generate TWO distinct claims and structure them as a list under the 'results' key:\n",
        "\n",
        "        1.  **Factual Claim:** A new, single sentence that is a concise factual statement directly supported by the CONTEXT and related to the ANSWER. Set 'label' to 'TRUE' and 'verdict' to true.\n",
        "        2.  **Misleading Claim:** A new, single sentence that directly contradicts a specific fact, date, or entity explicitly stated in the CONTEXT. Set 'label' to 'FALSE' and 'verdict' to false.\n",
        "\n",
        "        The 'rationale' for the FALSE claim must state the correct fact from the context. Ensure all claims, labels, and rationales are non-empty strings.\n",
        "    \"\"\").strip()\n",
        "\n",
        "    input_entries = []\n",
        "    for i, entry in enumerate(squad_batch_data):\n",
        "        entry_text = textwrap.dedent(f\"\"\"\n",
        "            --- Entry {i+1} ---\n",
        "            CONTEXT: {entry.get('context', 'N/A')}\n",
        "            QUESTION: {entry.get('question', 'N/A')}\n",
        "            CORRECT ANSWER: {entry.get('answer', 'N/A')}\n",
        "        \"\"\").strip()\n",
        "        input_entries.append(entry_text)\n",
        "\n",
        "    output_schema_dict = BatchFactCheckResult.model_json_schema()\n",
        "\n",
        "    prompt = (\n",
        "        f\"Process the following {len(squad_batch_data)} entries. Your final output MUST be a single JSON object that strictly conforms to the BatchFactCheckResult schema (which contains a list of claims). \"\n",
        "        \"The 'results' list must contain **exactly two** claims per entry: one TRUE and one FALSE. Do not include any text outside the JSON object.\\n\"\n",
        "        + \"\\n\\n\".join(input_entries)\n",
        "    )\n",
        "\n",
        "    config = types.GenerateContentConfig(\n",
        "            system_instruction=system_instruction,\n",
        "            response_mime_type=\"application/json\",\n",
        "            response_schema=output_schema_dict,\n",
        "            temperature=0.1,\n",
        "        )\n",
        "\n",
        "    return system_instruction, prompt, config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jplWBETW8Pfe"
      },
      "outputs": [],
      "source": [
        "def generate_squad_distillation_data(\n",
        "    squad_batch_data: List[Dict[str, str]],\n",
        "    teacher_model_name: str = 'gemini-2.5-pro'\n",
        ") -> List[Dict[str, Any]]:\n",
        "    if not squad_batch_data:\n",
        "        return []\n",
        "\n",
        "    system_instruction, prompt, config = _construct_distillation_prompt_and_config(\n",
        "        squad_batch_data, teacher_model_name\n",
        "    )\n",
        "\n",
        "    print(f\"-> Sending request for {len(squad_batch_data)} entries to teacher model ({teacher_model_name})...\")\n",
        "\n",
        "    client = genai.Client()\n",
        "    response = client.models.generate_content(\n",
        "        model=teacher_model_name,\n",
        "        contents=[system_instruction, prompt],\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    return json.loads(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABUpGd8k8Zpe"
      },
      "outputs": [],
      "source": [
        "def distill_squad_to_dataset(\n",
        "    input_dataset: Dataset,\n",
        "    batch_size: int = 50,\n",
        "    teacher_model_name: str = 'gemini-2.5-flash',\n",
        "    checkpoint_dir: str = 'distillation_checkpoints',\n",
        "    checkpoint_filename: str = 'squad_distilled_data.csv'\n",
        ") -> pd.DataFrame:\n",
        "    if not input_dataset:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_unprocessed, current_checkpoint_df = _get_unprocessed_squad_data(\n",
        "        input_dataset, checkpoint_dir, checkpoint_filename\n",
        "    )\n",
        "\n",
        "    if df_unprocessed.empty:\n",
        "        print(\"All SQuAD samples already processed. Returning final dataset.\")\n",
        "        return current_checkpoint_df\n",
        "\n",
        "    df_unprocessed = df_unprocessed.sample(frac=1).reset_index(drop=True)\n",
        "    total_samples = len(df_unprocessed)\n",
        "    batch_indices = np.array_split(df_unprocessed.index, np.ceil(total_samples / batch_size))\n",
        "    num_batches = len(batch_indices)\n",
        "    print(f\"SQuAD IDs remaining: {total_samples} in {num_batches} batches.\")\n",
        "\n",
        "    for i, indices in tqdm(enumerate(batch_indices), total=num_batches, desc=\"Distilling SQuAD Batches\"):\n",
        "        batch_df = df_unprocessed.loc[indices]\n",
        "\n",
        "        squad_batch_input = batch_df[[\n",
        "            'context', 'question', 'squad_answer'\n",
        "        ]].rename(columns={'squad_answer': 'answer'}).to_dict('records')\n",
        "\n",
        "        try:\n",
        "            distilled_batch_output = generate_squad_distillation_data(\n",
        "                squad_batch_data=squad_batch_input,\n",
        "                teacher_model_name=teacher_model_name\n",
        "            )\n",
        "\n",
        "            if not distilled_batch_output:\n",
        "                print(f\"Warning in Batch {i+1}: API returned empty results. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            batch_results_df = _process_and_flatten_batch_output(\n",
        "                distilled_batch_output['results'], batch_df\n",
        "            )\n",
        "\n",
        "            checkpoint(batch_results_df, checkpoint_dir, checkpoint_filename)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Critical Error in SQuAD Batch {i+1}. Halting distillation. Error: {e}\")\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        final_df = pd.read_csv(f\"{checkpoint_dir}/{checkpoint_filename}\")\n",
        "        final_df = final_df.drop_duplicates(subset=['claim']).reset_index(drop=True)\n",
        "        print(f\"SQuAD Distillation finished. Total final processed samples: {len(final_df)}\")\n",
        "        return final_df\n",
        "    except FileNotFoundError:\n",
        "        print(\"Final checkpoint file not found. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW-6ZFW08FCY"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B22Cd9spApan"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_PATH_FULL = '/content/drive/MyDrive/checkpoints/FactGuard/squad_distilled_data.csv'\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/checkpoints/FactGuard/'\n",
        "CHECKPOINT_FILENAME = 'squad_distilled_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdki9bo48ITs"
      },
      "outputs": [],
      "source": [
        "squad_dataset = load_dataset(\"squad\")\n",
        "train_split = squad_dataset['train']\n",
        "eval_split = squad_dataset['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--kQR5bN8bWx"
      },
      "outputs": [],
      "source": [
        "distilled_dataset = distill_squad_to_dataset(\n",
        "    train_split,\n",
        "    checkpoint_dir = CHECKPOINT_DIR,\n",
        "    checkpoint_filename = CHECKPOINT_FILENAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5obcaj-8ezN"
      },
      "source": [
        "### Push to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GksISU5R8ekD"
      },
      "outputs": [],
      "source": [
        "squad_dataset = load_checkpoint(CHECKPOINT_DIR, CHECKPOINT_FILENAME)\n",
        "squad_datasets = Dataset.from_pandas(squad_dataset)\n",
        "squad_dataset_dict = DatasetDict({\n",
        "    \"train\": squad_datasets\n",
        "})\n",
        "squad_dataset_dict.push_to_hub(\"factguard_squad_distilled_datasets\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
